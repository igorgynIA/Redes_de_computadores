# -*- coding: utf-8 -*-
"""Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n6NjoNLzojWTDE-Zc0hh8cevzPvr-MRN
"""

import torch
import torch.nn.functional as F
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoModel

# ============================
# 2. Toy Example
# ============================

# Frase simples
tokens = ["Eu", "amo", "Attention"]

# Embeddings fictícios (dimensão 3)
embeddings = torch.tensor([
    [1.0, 0.0, 1.0],   # Eu
    [0.0, 1.0, 1.0],   # amo
    [1.0, 1.0, 0.0]    # Attention
])

print("Embeddings iniciais:")
print(embeddings)

d_model = 3
d_k = 3

torch.manual_seed(42)

Wq = torch.randn(d_model, d_k)
Wk = torch.randn(d_model, d_k)
Wv = torch.randn(d_model, d_k)

Q = embeddings @ Wq
K = embeddings @ Wk
V = embeddings @ Wv

print("Q:\n", Q)
print("K:\n", K)
print("V:\n", V)

scores = Q @ K.T
print("Scores (QK^T):\n", scores)

scaled_scores = s